{"file_contents":{"src/etl_pipeline.py":{"content":"import time\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom utils.logger import ETLLogger\nfrom utils.config_loader import ConfigLoader\nfrom extractors.ftp_downloader import FTPDownloader\nfrom transformers.state_filter import StateFilter\nfrom loaders.spaces_uploader import SpacesUploader\n\n\nclass AttomETLPipeline:\n    \"\"\"Main ETL Pipeline orchestrator for Attom real estate data\"\"\"\n    \n    def __init__(self, config_file=\"config/config.json\"):\n        self.config_loader = ConfigLoader(config_file)\n        self.logger = ETLLogger()\n        \n        self.states = self.config_loader.get_states()\n        self.datasets = self.config_loader.get_datasets()\n        \n        self.logger.info(f\"Initialized ETL Pipeline\")\n        self.logger.info(f\"Target states: {', '.join(self.states)}\")\n        self.logger.info(f\"Datasets to process: {len(self.datasets)}\")\n        \n        self._initialize_components()\n    \n    def _initialize_components(self):\n        \"\"\"Initialize ETL components\"\"\"\n        working_dirs = self.config_loader.get_working_directories()\n        \n        ftp_config = self.config_loader.get_ftp_config()\n        ftp_config['download_dir'] = working_dirs.get('downloads', '/Outgoing')\n        self.downloader = FTPDownloader(self.logger, ftp_config)\n        \n        filter_config = {\n            'extracted_dir': working_dirs.get('extracted', 'data/extracted'),\n            'filtered_dir': working_dirs.get('filtered', 'data/filtered'),\n            'states': self.states,\n            'state_code_column': self.config_loader.get_state_code_column(),\n            'delimiter': self.config_loader.get_file_delimiter()\n        }\n        self.filter = StateFilter(self.logger, filter_config)\n        \n        spaces_config = self.config_loader.get_spaces_config()\n        self.uploader = SpacesUploader(self.logger, spaces_config)\n    \n    def process_dataset(self, dataset_config):\n        \"\"\"\n        Process a single dataset through the full ETL pipeline\n        \n        Args:\n            dataset_config: Dataset configuration dict with 'name', 'urls', 'enabled'\n        \n        Returns:\n            Dict with processing results\n        \"\"\"\n        dataset_name = dataset_config.get('name')\n        \n        if not dataset_config.get('enabled', True):\n            self.logger.info(f\"Dataset {dataset_name} is disabled, skipping...\")\n            return {'status': 'skipped', 'dataset': dataset_name}\n        \n        start_time = time.time()\n        self.logger.log_etl_start(dataset_name)\n        \n        try:\n            downloaded_files = self.downloader.download_dataset(dataset_config)\n            \n            if not downloaded_files:\n                self.logger.warning(f\"No files downloaded for {dataset_name}\")\n                return {'status': 'no_files', 'dataset': dataset_name}\n            \n            all_filtered_zips = []\n            \n            for zip_file in downloaded_files:\n                try:\n                    extracted_files = self.filter.extract_zip(zip_file)\n                    \n                    for extracted_file in extracted_files:\n                        if extracted_file.suffix.lower() in ['.txt', '.csv']:\n                            filtered_files = self.filter.filter_multiple_states(extracted_file)\n                            \n                            if filtered_files:\n                                zip_name = f\"{dataset_name}_{extracted_file.stem}_{datetime.now().strftime('%Y%m%d')}.zip\"\n                                filtered_zip = self.filter.compress_to_zip(filtered_files, zip_name)\n                                all_filtered_zips.append(filtered_zip)\n                                \n                                for filtered_file in filtered_files:\n                                    if filtered_file.exists():\n                                        filtered_file.unlink()\n                        \n                        if extracted_file.exists():\n                            extracted_file.unlink()\n                    \n                    if zip_file.exists():\n                        zip_file.unlink()\n                        \n                except Exception as e:\n                    self.logger.error(f\"Error processing {zip_file.name}: {e}\")\n            \n            uploaded_urls = []\n            if all_filtered_zips:\n                uploaded_urls = self.uploader.upload_multiple_files(all_filtered_zips)\n                \n                for filtered_zip in all_filtered_zips:\n                    if filtered_zip.exists():\n                        filtered_zip.unlink()\n            \n            duration = time.time() - start_time\n            self.logger.log_etl_end(dataset_name, duration)\n            \n            return {\n                'status': 'success',\n                'dataset': dataset_name,\n                'uploaded_files': len(uploaded_urls),\n                'urls': uploaded_urls,\n                'duration': duration\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"ETL pipeline failed for {dataset_name}: {e}\")\n            duration = time.time() - start_time\n            self.logger.log_etl_end(dataset_name, duration)\n            \n            return {\n                'status': 'failed',\n                'dataset': dataset_name,\n                'error': str(e),\n                'duration': duration\n            }\n    \n    def run_all_datasets(self):\n        \"\"\"\n        Run ETL pipeline for all configured datasets\n        \n        Returns:\n            Dict with overall results\n        \"\"\"\n        self.logger.info(\"=\"*80)\n        self.logger.info(\"STARTING FULL ETL PIPELINE RUN\")\n        self.logger.info(\"=\"*80)\n        \n        overall_start = time.time()\n        results = []\n        \n        for dataset_config in self.datasets:\n            result = self.process_dataset(dataset_config)\n            results.append(result)\n        \n        overall_duration = time.time() - overall_start\n        \n        success_count = sum(1 for r in results if r['status'] == 'success')\n        failed_count = sum(1 for r in results if r['status'] == 'failed')\n        skipped_count = sum(1 for r in results if r['status'] in ['skipped', 'no_files'])\n        \n        self.logger.info(\"=\"*80)\n        self.logger.info(\"ETL PIPELINE RUN COMPLETE\")\n        self.logger.info(f\"Total datasets: {len(results)}\")\n        self.logger.info(f\"Successful: {success_count}\")\n        self.logger.info(f\"Failed: {failed_count}\")\n        self.logger.info(f\"Skipped: {skipped_count}\")\n        self.logger.info(f\"Total duration: {overall_duration:.2f} seconds\")\n        self.logger.info(\"=\"*80)\n        \n        try:\n            log_file = self.logger.get_log_file_path()\n            if os.path.exists(log_file):\n                self.uploader.upload_log_file(log_file)\n        except Exception as e:\n            self.logger.warning(f\"Failed to upload log file: {e}\")\n        \n        return {\n            'total': len(results),\n            'success': success_count,\n            'failed': failed_count,\n            'skipped': skipped_count,\n            'duration': overall_duration,\n            'results': results\n        }\n\n\ndef main():\n    \"\"\"Main entry point for running ETL pipeline\"\"\"\n    try:\n        pipeline = AttomETLPipeline()\n        results = pipeline.run_all_datasets()\n        \n        if results['failed'] > 0:\n            exit(1)\n        else:\n            exit(0)\n            \n    except Exception as e:\n        print(f\"FATAL ERROR: {e}\")\n        exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":7565},"validate_setup.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nSetup validation script for Attom ETL Pipeline\nValidates configuration and displays system information\n\"\"\"\n\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent / 'src'))\n\ndef validate_setup():\n    \"\"\"Validate the ETL pipeline setup\"\"\"\n    print(\"=\"*80)\n    print(\"ATTOM ETL PIPELINE - SETUP VALIDATION\")\n    print(\"=\"*80)\n    print()\n    \n    errors = []\n    warnings = []\n    \n    print(\"1. Checking directory structure...\")\n    required_dirs = ['src', 'config', 'logs', 'data/downloads', 'data/extracted', \n                     'data/filtered', 'data/temp']\n    for dir_path in required_dirs:\n        if Path(dir_path).exists():\n            print(f\"   ✓ {dir_path}\")\n        else:\n            print(f\"   ✗ {dir_path} - MISSING\")\n            errors.append(f\"Missing directory: {dir_path}\")\n    print()\n    \n    print(\"2. Checking Python modules...\")\n    modules_ok = True\n    try:\n        import boto3\n        print(\"   ✓ boto3 (DigitalOcean Spaces)\")\n    except ImportError:\n        print(\"   ✗ boto3 - NOT INSTALLED\")\n        errors.append(\"boto3 not installed\")\n        modules_ok = False\n    \n    try:\n        import requests\n        print(\"   ✓ requests (HTTP downloads)\")\n    except ImportError:\n        print(\"   ✗ requests - NOT INSTALLED\")\n        errors.append(\"requests not installed\")\n        modules_ok = False\n    \n    try:\n        import schedule\n        print(\"   ✓ schedule (Job scheduling)\")\n    except ImportError:\n        print(\"   ✗ schedule - NOT INSTALLED\")\n        errors.append(\"schedule not installed\")\n        modules_ok = False\n    \n    try:\n        from dotenv import load_dotenv\n        print(\"   ✓ python-dotenv (Environment variables)\")\n    except ImportError:\n        print(\"   ✗ python-dotenv - NOT INSTALLED\")\n        errors.append(\"python-dotenv not installed\")\n        modules_ok = False\n    print()\n    \n    print(\"3. Checking configuration files...\")\n    config_file = Path('config/config.json')\n    if config_file.exists():\n        print(\"   ✓ config/config.json\")\n        try:\n            import json\n            with open(config_file, 'r') as f:\n                config = json.load(f)\n            print(f\"   ✓ Configuration is valid JSON\")\n            print(f\"      - States configured: {len(config.get('states', []))}\")\n            print(f\"      - Datasets configured: {len(config.get('datasets', []))}\")\n            print(f\"      - Schedule time: {config.get('schedule', {}).get('daily_time', 'Not set')}\")\n        except Exception as e:\n            print(f\"   ✗ Configuration parsing failed: {e}\")\n            errors.append(f\"Config parsing error: {e}\")\n    else:\n        print(\"   ✗ config/config.json - MISSING\")\n        errors.append(\"Missing config.json\")\n    \n    env_example = Path('config/.env.example')\n    if env_example.exists():\n        print(\"   ✓ config/.env.example\")\n    else:\n        print(\"   ⚠ config/.env.example - MISSING\")\n        warnings.append(\"Missing .env.example template\")\n    \n    import os\n    env_file = Path('config/.env')\n    spaces_key = os.getenv('SPACES_ACCESS_KEY')\n    spaces_secret = os.getenv('SPACES_SECRET_KEY')\n    ftp_user = os.getenv('FTP_USERNAME')\n    ftp_pass = os.getenv('FTP_PASSWORD')\n    \n    has_env_file = env_file.exists()\n    has_env_vars = all([spaces_key, spaces_secret, ftp_user, ftp_pass])\n    \n    if has_env_vars:\n        print(\"   ✓ Credentials configured (via environment variables)\")\n        print(\"      - SPACES_ACCESS_KEY: Set\")\n        print(\"      - SPACES_SECRET_KEY: Set\")\n        print(\"      - FTP_USERNAME: Set\")\n        print(\"      - FTP_PASSWORD: Set\")\n    elif has_env_file:\n        print(\"   ✓ config/.env (credentials file exists)\")\n    else:\n        print(\"   ⚠ Credentials not configured\")\n        warnings.append(\"Credentials not configured (neither .env file nor environment variables)\")\n    print()\n    \n    print(\"4. Checking ETL modules...\")\n    try:\n        from utils.logger import ETLLogger\n        print(\"   ✓ Logger module\")\n    except Exception as e:\n        print(f\"   ✗ Logger module failed: {e}\")\n        errors.append(\"Logger module error\")\n    \n    try:\n        from utils.config_loader import ConfigLoader\n        print(\"   ✓ Config loader module\")\n    except Exception as e:\n        print(f\"   ✗ Config loader module failed: {e}\")\n        errors.append(\"Config loader error\")\n    \n    try:\n        from extractors.ftp_downloader import FTPDownloader\n        print(\"   ✓ FTP downloader module\")\n    except Exception as e:\n        print(f\"   ✗ FTP downloader module failed: {e}\")\n        errors.append(\"FTP downloader error\")\n    \n    try:\n        from transformers.state_filter import StateFilter\n        print(\"   ✓ State filter module\")\n    except Exception as e:\n        print(f\"   ✗ State filter module failed: {e}\")\n        errors.append(\"State filter error\")\n    \n    try:\n        from loaders.spaces_uploader import SpacesUploader\n        print(\"   ✓ Spaces uploader module\")\n    except Exception as e:\n        print(f\"   ✗ Spaces uploader module failed: {e}\")\n        errors.append(\"Spaces uploader error\")\n    \n    try:\n        from etl_pipeline import AttomETLPipeline\n        print(\"   ✓ ETL pipeline orchestrator\")\n    except Exception as e:\n        print(f\"   ✗ ETL pipeline orchestrator failed: {e}\")\n        errors.append(\"ETL pipeline error\")\n    \n    try:\n        from scheduler import ETLScheduler\n        print(\"   ✓ Scheduler module\")\n    except Exception as e:\n        print(f\"   ✗ Scheduler module failed: {e}\")\n        errors.append(\"Scheduler error\")\n    print()\n    \n    print(\"=\"*80)\n    print(\"VALIDATION SUMMARY\")\n    print(\"=\"*80)\n    \n    if errors:\n        print(f\"\\n❌ ERRORS FOUND ({len(errors)}):\")\n        for error in errors:\n            print(f\"   - {error}\")\n    \n    if warnings:\n        print(f\"\\n⚠️  WARNINGS ({len(warnings)}):\")\n        for warning in warnings:\n            print(f\"   - {warning}\")\n    \n    if not errors and not warnings:\n        print(\"\\n✅ ALL CHECKS PASSED!\")\n        print(\"\\nThe ETL pipeline is properly configured and ready to use.\")\n        print(\"\\nNext steps:\")\n        print(\"1. Configure credentials in config/.env\")\n        print(\"2. Update config/config.json with your Attom FTP URLs\")\n        print(\"3. Test with: python main.py run\")\n        print(\"4. Deploy with: python main.py schedule\")\n    elif not errors:\n        print(\"\\n✅ SETUP COMPLETE WITH WARNINGS\")\n        print(\"\\nThe application is functional but has some warnings.\")\n        print(\"Review the warnings above and configure as needed.\")\n    else:\n        print(\"\\n❌ SETUP INCOMPLETE\")\n        print(\"\\nPlease fix the errors above before using the application.\")\n        return 1\n    \n    print(\"=\"*80)\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(validate_setup())\n","size_bytes":6893},"src/loaders/spaces_uploader.py":{"content":"import boto3\nfrom botocore.exceptions import ClientError\nfrom pathlib import Path\nfrom datetime import datetime\n\n\nclass SpacesUploader:\n    \"\"\"Upload files to DigitalOcean Spaces (S3-compatible storage)\"\"\"\n    \n    def __init__(self, logger, config):\n        self.logger = logger\n        self.config = config\n        \n        self.access_key = config.get('access_key')\n        self.secret_key = config.get('secret_key')\n        self.region = config.get('region', 'sfo3')\n        self.bucket_name = config.get('bucket_name')\n        self.endpoint_url = config.get('endpoint_url', f'https://{self.region}.digitaloceanspaces.com')\n        \n        if not self.access_key or not self.secret_key:\n            raise ValueError(\"Spaces access_key and secret_key must be provided\")\n        \n        if not self.bucket_name:\n            raise ValueError(\"Spaces bucket_name must be provided\")\n        \n        self.s3_client = boto3.client(\n            's3',\n            region_name=self.region,\n            endpoint_url=self.endpoint_url,\n            aws_access_key_id=self.access_key,\n            aws_secret_access_key=self.secret_key\n        )\n        \n        self.logger.info(f\"Initialized Spaces uploader for bucket: {self.bucket_name}\")\n    \n    def upload_file(self, local_file_path, remote_path=None, make_public=False):\n        \"\"\"\n        Upload a file to DigitalOcean Spaces\n        \n        Args:\n            local_file_path: Path to local file\n            remote_path: Remote path in bucket (optional, uses date-based structure if not provided)\n            make_public: Whether to make the file publicly accessible\n        \n        Returns:\n            Public URL of uploaded file\n        \"\"\"\n        local_file = Path(local_file_path)\n        \n        if not local_file.exists():\n            raise FileNotFoundError(f\"Local file not found: {local_file_path}\")\n        \n        if not remote_path:\n            today = datetime.now().strftime('%Y-%m-%d')\n            remote_path = f\"{today}/{local_file.name}\"\n        \n        self.logger.info(f\"Uploading {local_file.name} to Spaces: {remote_path}\")\n        \n        try:\n            extra_args = {}\n            if make_public:\n                extra_args['ACL'] = 'public-read'\n            \n            self.s3_client.upload_file(\n                str(local_file),\n                self.bucket_name,\n                remote_path,\n                ExtraArgs=extra_args\n            )\n            \n            public_url = f\"{self.endpoint_url}/{self.bucket_name}/{remote_path}\"\n            \n            self.logger.log_upload_progress(local_file.name, \"Success\")\n            self.logger.info(f\"Upload complete: {public_url}\")\n            \n            return public_url\n            \n        except ClientError as e:\n            self.logger.error(f\"Failed to upload {local_file.name}: {e}\")\n            raise\n    \n    def upload_multiple_files(self, file_paths, folder_prefix=None, make_public=False):\n        \"\"\"\n        Upload multiple files to Spaces\n        \n        Args:\n            file_paths: List of local file paths\n            folder_prefix: Optional folder prefix (uses date if not provided)\n            make_public: Whether to make files publicly accessible\n        \n        Returns:\n            List of public URLs\n        \"\"\"\n        if not folder_prefix:\n            folder_prefix = datetime.now().strftime('%Y-%m-%d')\n        \n        uploaded_urls = []\n        \n        for file_path in file_paths:\n            try:\n                local_file = Path(file_path)\n                remote_path = f\"{folder_prefix}/{local_file.name}\"\n                url = self.upload_file(file_path, remote_path, make_public)\n                uploaded_urls.append(url)\n            except Exception as e:\n                self.logger.error(f\"Failed to upload {file_path}: {e}\")\n        \n        return uploaded_urls\n    \n    def list_files(self, prefix=''):\n        \"\"\"\n        List files in the bucket\n        \n        Args:\n            prefix: Optional prefix to filter files\n        \n        Returns:\n            List of file keys\n        \"\"\"\n        try:\n            response = self.s3_client.list_objects_v2(\n                Bucket=self.bucket_name,\n                Prefix=prefix\n            )\n            \n            if 'Contents' in response:\n                return [obj['Key'] for obj in response['Contents']]\n            else:\n                return []\n                \n        except ClientError as e:\n            self.logger.error(f\"Failed to list files: {e}\")\n            return []\n    \n    def delete_file(self, remote_path):\n        \"\"\"\n        Delete a file from Spaces\n        \n        Args:\n            remote_path: Remote file path in bucket\n        \"\"\"\n        try:\n            self.s3_client.delete_object(\n                Bucket=self.bucket_name,\n                Key=remote_path\n            )\n            self.logger.info(f\"Deleted from Spaces: {remote_path}\")\n        except ClientError as e:\n            self.logger.error(f\"Failed to delete {remote_path}: {e}\")\n            raise\n    \n    def upload_log_file(self, log_file_path):\n        \"\"\"\n        Upload log file to Spaces\n        \n        Args:\n            log_file_path: Path to log file\n        \n        Returns:\n            Public URL of uploaded log\n        \"\"\"\n        today = datetime.now().strftime('%Y-%m-%d')\n        log_folder = f\"logs/{today}\"\n        \n        return self.upload_file(log_file_path, f\"{log_folder}/{Path(log_file_path).name}\")\n","size_bytes":5474},"README.md":{"content":"# Attom Real Estate Data ETL Pipeline\n\nAn automated Python ETL pipeline for processing 142+ GB of Attom nationwide real estate data, filtering by state codes, and uploading to DigitalOcean Spaces for consumption by .NET applications.\n\n## Overview\n\nThis application automatically:\n- Downloads large ZIP files (2GB+) from Attom's FTP/HTTP data sources\n- Extracts and streams TXT files line-by-line for memory efficiency\n- Filters records by state code (SitusStateCode column)\n- Compresses filtered data back into ZIP archives\n- Uploads to DigitalOcean Spaces with date-based folder structure\n- Logs all operations comprehensively\n- Runs on a daily schedule\n\n## Features\n\n- **Memory-Efficient Processing**: Streams 2GB+ files line-by-line without loading into memory\n- **Multi-State Filtering**: Filter data for multiple states (CA, TX, FL, etc.)\n- **Automated Scheduling**: Daily execution at configurable time (default: 2:00 AM)\n- **Comprehensive Logging**: Tracks downloads, record counts, errors, and processing time\n- **DigitalOcean Spaces Integration**: S3-compatible cloud storage with date-based organization\n- **Configurable Datasets**: Support for Assessor, AVM, Parcel, PROPERTYTOBOUNDARYMATCH_PARCEL, Recorder, and PreForeclosure\n- **Error Handling**: Robust error handling with detailed logging\n\n## Project Structure\n\n```\n.\n├── src/\n│   ├── extractors/\n│   │   └── ftp_downloader.py       # FTP/HTTP download module\n│   ├── transformers/\n│   │   └── state_filter.py         # Streaming filter for state-based filtering\n│   ├── loaders/\n│   │   └── spaces_uploader.py      # DigitalOcean Spaces uploader\n│   ├── utils/\n│   │   ├── logger.py               # Comprehensive logging system\n│   │   └── config_loader.py        # Configuration management\n│   ├── etl_pipeline.py             # Main ETL orchestrator\n│   └── scheduler.py                # Daily scheduling system\n├── config/\n│   ├── config.json                 # Main configuration file\n│   └── .env.example                # Environment variables template\n├── data/\n│   ├── downloads/                  # Downloaded ZIP files (temporary)\n│   ├── extracted/                  # Extracted TXT files (temporary)\n│   ├── filtered/                   # Filtered files (temporary)\n│   └── temp/                       # Temporary working directory\n├── logs/                           # ETL process logs\n├── main.py                         # Application entry point\n└── README.md                       # This file\n```\n\n## Installation & Setup\n\n### 1. Prerequisites\n\n- Python 3.11+\n- DigitalOcean Droplet (or any Linux server)\n- DigitalOcean Spaces bucket\n- Access to Attom data sources (FTP credentials)\n\n### 2. Install Dependencies\n\nAll dependencies are already installed via `uv`:\n- boto3 (DigitalOcean Spaces/S3)\n- requests (HTTP downloads)\n- schedule (Job scheduling)\n- python-dotenv (Environment configuration)\n\n### 3. Configure the Application\n\n#### a. Copy and edit the environment file:\n\n```bash\ncp config/.env.example config/.env\n```\n\nEdit `config/.env` and add your credentials:\n```\nSPACES_ACCESS_KEY=your_digitalocean_spaces_access_key\nSPACES_SECRET_KEY=your_digitalocean_spaces_secret_key\nFTP_USERNAME=your_attom_ftp_username\nFTP_PASSWORD=your_attom_ftp_password\n```\n\n#### b. Edit configuration file:\n\nEdit `config/config.json` to customize:\n- **states**: List of state codes to filter (e.g., [\"CA\", \"TX\", \"FL\"])\n- **datasets**: Configure dataset URLs and enable/disable specific datasets\n- **spaces.bucket_name**: Your DigitalOcean Spaces bucket name\n- **schedule.daily_time**: Time to run daily (format: \"HH:MM\", 24-hour)\n\nExample `config.json`:\n```json\n{\n  \"states\": [\"CA\", \"TX\", \"FL\"],\n  \"datasets\": [\n    {\n      \"name\": \"Assessor\",\n      \"enabled\": true,\n      \"urls\": [\"ftp://ftp.attom.com/path/to/Assessor_National.zip\"]\n    }\n  ],\n  \"spaces\": {\n    \"bucket_name\": \"genieattomdata\",\n    \"region\": \"sfo3\"\n  },\n  \"schedule\": {\n    \"daily_time\": \"02:00\"\n  }\n}\n```\n\n## Usage\n\n### Run ETL Pipeline Once (Manual)\n\n```bash\npython src/etl_pipeline.py\n```\n\n### Run with Scheduler (Automated Daily)\n\n```bash\npython src/scheduler.py\n```\n\nRun immediately and then on schedule:\n```bash\npython src/scheduler.py --now\n```\n\n### Using the Main Entry Point\n\n```bash\n# Run once\npython main.py run\n\n# Run with scheduler\npython main.py schedule\n\n# Run immediately and then on schedule\npython main.py schedule --now\n```\n\n## Deployment to Production\n\n### Option 1: Using systemd (Recommended)\n\nCreate a systemd service file `/etc/systemd/system/attom-etl.service`:\n\n```ini\n[Unit]\nDescription=Attom ETL Pipeline Scheduler\nAfter=network.target\n\n[Service]\nType=simple\nUser=your-user\nWorkingDirectory=/path/to/attom-etl\nExecStart=/usr/bin/python3 /path/to/attom-etl/src/scheduler.py\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n```\n\nEnable and start:\n```bash\nsudo systemctl daemon-reload\nsudo systemctl enable attom-etl\nsudo systemctl start attom-etl\nsudo systemctl status attom-etl\n```\n\n### Option 2: Using Cron\n\nAdd to crontab:\n```bash\n0 2 * * * cd /path/to/attom-etl && python3 src/etl_pipeline.py >> logs/cron.log 2>&1\n```\n\n### Option 3: Using PM2 (Node.js process manager)\n\n```bash\npm2 start src/scheduler.py --name attom-etl --interpreter python3\npm2 save\npm2 startup\n```\n\n## Output Structure\n\nFiltered files are uploaded to DigitalOcean Spaces with this structure:\n\n```\nspaces://genieattomdata.sfo3.digitaloceanspaces.com/\n├── 2025-10-29/\n│   ├── Assessor_National_filtered_CA_20251029.zip\n│   ├── Assessor_National_filtered_TX_20251029.zip\n│   ├── Assessor_National_filtered_FL_20251029.zip\n│   ├── AVM_National_filtered_CA_20251029.zip\n│   └── ...\n├── 2025-10-30/\n│   └── ...\n└── logs/\n    └── 2025-10-29/\n        └── etl_20251029_020000.log\n```\n\n## Monitoring\n\n### View Logs\n\n```bash\n# View latest log\ntail -f logs/etl_*.log\n\n# View all logs\nls -lh logs/\n```\n\n### Check systemd Service\n\n```bash\nsudo systemctl status attom-etl\nsudo journalctl -u attom-etl -f\n```\n\n## Configuration Reference\n\n### Dataset Priority Levels\n\n**High Priority** (enabled by default):\n- Assessor\n- AVM\n- Parcel (via Jetstream)\n- PROPERTYTOBOUNDARYMATCH_PARCEL\n\n**Lower Priority** (disabled by default):\n- Recorder\n- PreForeclosure\n\nEnable/disable datasets by setting `\"enabled\": true/false` in `config.json`.\n\n### State Codes\n\nAdd any US state codes to the `states` array:\n```json\n\"states\": [\"CA\", \"TX\", \"FL\", \"NY\", \"IL\", \"OH\", \"PA\"]\n```\n\n### Schedule Configuration\n\nThe scheduler uses 24-hour format:\n```json\n\"schedule\": {\n  \"daily_time\": \"02:00\"  // 2:00 AM daily\n}\n```\n\n## Troubleshooting\n\n### Issue: FTP download fails\n\n- Verify FTP credentials in `.env`\n- Check firewall allows FTP connections\n- Test FTP access manually with `ftp` or FileZilla\n\n### Issue: Out of memory errors\n\n- The pipeline streams files line-by-line, so this should not occur\n- Verify sufficient disk space in `data/` directories\n- Check system resources with `df -h` and `free -m`\n\n### Issue: Upload to Spaces fails\n\n- Verify Spaces credentials in `.env`\n- Check bucket name matches in `config.json`\n- Verify network connectivity to DigitalOcean\n\n### Issue: Scheduler not running\n\n- Check systemd service status: `sudo systemctl status attom-etl`\n- View logs: `sudo journalctl -u attom-etl`\n- Verify Python path in systemd service file\n\n## Future Enhancements\n\n- Email notifications for job failures\n- Web dashboard for monitoring\n- Parallel processing of multiple datasets\n- Incremental updates (only download changed files)\n- Data validation and integrity checks\n- Retry logic with exponential backoff\n\n## Support\n\nFor issues or questions, check the logs in `logs/` directory for detailed error messages and processing information.\n\n## License\n\nProprietary - Internal use only\n","size_bytes":7973},"src/utils/logger.py":{"content":"import logging\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\n\nclass ETLLogger:\n    \"\"\"Comprehensive logging system for ETL pipeline\"\"\"\n    \n    def __init__(self, log_dir=\"logs\", log_level=logging.INFO):\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(exist_ok=True)\n        \n        self.log_file = self.log_dir / f\"etl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n        \n        self.logger = logging.getLogger('AttomETL')\n        self.logger.setLevel(log_level)\n        \n        self.logger.handlers.clear()\n        \n        file_handler = logging.FileHandler(self.log_file, encoding='utf-8')\n        file_handler.setLevel(log_level)\n        \n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(log_level)\n        \n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n            datefmt='%Y-%m-%d %H:%M:%S'\n        )\n        file_handler.setFormatter(formatter)\n        console_handler.setFormatter(formatter)\n        \n        self.logger.addHandler(file_handler)\n        self.logger.addHandler(console_handler)\n        \n    def info(self, message):\n        self.logger.info(message)\n    \n    def warning(self, message):\n        self.logger.warning(message)\n    \n    def error(self, message):\n        self.logger.error(message)\n    \n    def debug(self, message):\n        self.logger.debug(message)\n    \n    def log_etl_start(self, dataset_name):\n        self.info(f\"{'='*80}\")\n        self.info(f\"Starting ETL process for dataset: {dataset_name}\")\n        self.info(f\"{'='*80}\")\n    \n    def log_etl_end(self, dataset_name, duration):\n        self.info(f\"{'='*80}\")\n        self.info(f\"Completed ETL process for dataset: {dataset_name}\")\n        self.info(f\"Total duration: {duration:.2f} seconds\")\n        self.info(f\"{'='*80}\")\n    \n    def log_download_progress(self, filename, downloaded, total):\n        if total > 0:\n            percentage = (downloaded / total) * 100\n            self.info(f\"Downloading {filename}: {downloaded}/{total} bytes ({percentage:.1f}%)\")\n        else:\n            self.info(f\"Downloading {filename}: {downloaded} bytes\")\n    \n    def log_filter_progress(self, filename, records_processed, records_kept):\n        self.info(f\"Filtering {filename}: Processed {records_processed} records, Kept {records_kept} records\")\n    \n    def log_upload_progress(self, filename, status):\n        self.info(f\"Upload {filename}: {status}\")\n    \n    def get_log_file_path(self):\n        return str(self.log_file)\n","size_bytes":2559},"DEPLOYMENT.md":{"content":"# Deployment Guide - Attom ETL Pipeline\n\nThis guide walks through deploying the Attom ETL Pipeline to a DigitalOcean Droplet in production.\n\n## Prerequisites\n\n1. **DigitalOcean Droplet** running Ubuntu 20.04 or later\n2. **DigitalOcean Spaces** bucket created\n3. **Attom FTP credentials** (username and password)\n4. **SSH access** to your droplet\n\n## Step 1: Prepare the Droplet\n\nSSH into your droplet:\n```bash\nssh root@your_droplet_ip\n```\n\nUpdate system packages:\n```bash\napt update && apt upgrade -y\n```\n\nInstall Python 3.11 and required tools:\n```bash\napt install -y python3.11 python3.11-venv python3-pip git\n```\n\n## Step 2: Clone or Upload the Application\n\n### Option A: Via Git (if using version control)\n```bash\ncd /opt\ngit clone https://github.com/yourusername/attom-etl.git\ncd attom-etl\n```\n\n### Option B: Via SCP (upload from local machine)\n```bash\n# From your local machine\nscp -r /path/to/attom-etl root@your_droplet_ip:/opt/\n```\n\n## Step 3: Install Python Dependencies\n\n```bash\ncd /opt/attom-etl\n\n# Install uv (fast Python package manager)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nsource $HOME/.cargo/env\n\n# Install dependencies\nuv venv\nsource .venv/bin/activate\nuv pip install boto3 requests schedule python-dotenv\n```\n\n## Step 4: Configure the Application\n\n### Create .env file with credentials:\n\n```bash\nnano config/.env\n```\n\nAdd your credentials:\n```\nSPACES_ACCESS_KEY=your_spaces_access_key_here\nSPACES_SECRET_KEY=your_spaces_secret_key_here\nFTP_USERNAME=your_ftp_username_here\nFTP_PASSWORD=your_ftp_password_here\n```\n\nSave and exit (Ctrl+X, Y, Enter).\n\n### Edit config.json:\n\n```bash\nnano config/config.json\n```\n\nUpdate the following:\n- `states`: Add your target state codes\n- `datasets[].urls`: Update with actual Attom FTP URLs\n- `spaces.bucket_name`: Your DigitalOcean Spaces bucket name\n- `schedule.daily_time`: Desired execution time\n\n## Step 5: Test the Application\n\nRun a test to ensure everything works:\n\n```bash\npython3 main.py run\n```\n\nCheck the logs:\n```bash\ntail -f logs/etl_*.log\n```\n\n## Step 6: Set Up Systemd Service (Production)\n\nCreate systemd service file:\n```bash\nnano /etc/systemd/system/attom-etl.service\n```\n\nAdd the following content:\n```ini\n[Unit]\nDescription=Attom ETL Pipeline Scheduler\nAfter=network.target\n\n[Service]\nType=simple\nUser=root\nWorkingDirectory=/opt/attom-etl\nEnvironment=\"PATH=/opt/attom-etl/.venv/bin:/usr/local/bin:/usr/bin:/bin\"\nExecStart=/opt/attom-etl/.venv/bin/python3 /opt/attom-etl/src/scheduler.py\nRestart=always\nRestartSec=10\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\n```\n\nSave and exit.\n\nEnable and start the service:\n```bash\nsystemctl daemon-reload\nsystemctl enable attom-etl\nsystemctl start attom-etl\nsystemctl status attom-etl\n```\n\n## Step 7: Monitor the Application\n\n### View service status:\n```bash\nsystemctl status attom-etl\n```\n\n### View logs:\n```bash\n# System logs\njournalctl -u attom-etl -f\n\n# Application logs\ntail -f /opt/attom-etl/logs/etl_*.log\n```\n\n### Check disk space:\n```bash\ndf -h\n```\n\n### Monitor running processes:\n```bash\nhtop\n```\n\n## Step 8: Verify DigitalOcean Spaces\n\nCheck that files are being uploaded:\n\n1. Go to DigitalOcean Control Panel\n2. Navigate to Spaces\n3. Open your bucket\n4. Look for dated folders (e.g., `2025-10-29/`)\n5. Verify ZIP files are present\n\n## Maintenance\n\n### Manually trigger ETL:\n```bash\ncd /opt/attom-etl\npython3 main.py run\n```\n\n### Restart service:\n```bash\nsystemctl restart attom-etl\n```\n\n### View recent logs:\n```bash\nls -lh logs/\ntail -100 logs/etl_*.log\n```\n\n### Clean up old data:\n```bash\n# Remove downloaded files older than 7 days\nfind /opt/attom-etl/data/downloads -type f -mtime +7 -delete\nfind /opt/attom-etl/data/extracted -type f -mtime +7 -delete\nfind /opt/attom-etl/data/filtered -type f -mtime +7 -delete\n\n# Remove old logs (older than 30 days)\nfind /opt/attom-etl/logs -type f -mtime +30 -delete\n```\n\n### Update application:\n```bash\ncd /opt/attom-etl\ngit pull  # if using git\nsystemctl restart attom-etl\n```\n\n## Troubleshooting\n\n### Service won't start:\n```bash\n# Check status\nsystemctl status attom-etl\n\n# View detailed logs\njournalctl -u attom-etl -n 100\n\n# Check Python path\nwhich python3\n\n# Test manually\ncd /opt/attom-etl\npython3 src/scheduler.py\n```\n\n### Out of disk space:\n```bash\n# Check space\ndf -h\n\n# Clean up data directories\nrm -rf /opt/attom-etl/data/downloads/*\nrm -rf /opt/attom-etl/data/extracted/*\nrm -rf /opt/attom-etl/data/filtered/*\n```\n\n### FTP connection issues:\n```bash\n# Test FTP manually\nftp ftp.attom.com\n# Enter username and password\n# Try to navigate directories\n```\n\n### Spaces upload fails:\n```bash\n# Test credentials\npython3 -c \"\nimport boto3\ns3 = boto3.client('s3', \n    endpoint_url='https://sfo3.digitaloceanspaces.com',\n    aws_access_key_id='YOUR_KEY',\n    aws_secret_access_key='YOUR_SECRET')\nprint(s3.list_buckets())\n\"\n```\n\n## Security Best Practices\n\n1. **Never commit credentials** to version control\n2. **Use environment variables** for all secrets\n3. **Restrict file permissions**:\n   ```bash\n   chmod 600 config/.env\n   chmod 700 config/\n   ```\n4. **Regular backups** of configuration files\n5. **Monitor logs** for unauthorized access attempts\n\n## Scaling Considerations\n\nFor larger deployments:\n- Consider using multiple droplets for different datasets\n- Implement parallel processing for multiple states\n- Use a message queue (Redis/RabbitMQ) for job management\n- Set up monitoring with Datadog or New Relic\n- Implement alerting for failures\n\n## Support\n\nCheck application logs for detailed error messages:\n```bash\ntail -f logs/etl_*.log\n```\n\nView the main README.md for more information.\n","size_bytes":5621},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"boto3>=1.40.61\",\n    \"python-dotenv>=1.2.1\",\n    \"requests>=2.32.5\",\n    \"schedule>=1.2.2\",\n]\n","size_bytes":241},"src/transformers/state_filter.py":{"content":"import zipfile\nimport os\nfrom pathlib import Path\nimport time\n\n\nclass StateFilter:\n    \"\"\"Memory-efficient streaming filter for large TXT files\"\"\"\n    \n    def __init__(self, logger, config):\n        self.logger = logger\n        self.config = config\n        self.extracted_dir = Path(config.get('extracted_dir', 'data/extracted'))\n        self.filtered_dir = Path(config.get('filtered_dir', 'data/filtered'))\n        self.extracted_dir.mkdir(parents=True, exist_ok=True)\n        self.filtered_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.states = config.get('states', [])\n        self.state_code_column = config.get('state_code_column', 'SitusStateCode')\n        self.delimiter = config.get('delimiter', '\\t')\n    \n    def extract_zip(self, zip_path):\n        \"\"\"\n        Extract ZIP file to working directory\n        \n        Args:\n            zip_path: Path to ZIP file\n        \n        Returns:\n            List of extracted file paths\n        \"\"\"\n        self.logger.info(f\"Extracting ZIP file: {zip_path.name}\")\n        \n        extracted_files = []\n        \n        try:\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                members = zip_ref.namelist()\n                \n                for member in members:\n                    extract_path = self.extracted_dir / member\n                    zip_ref.extract(member, self.extracted_dir)\n                    extracted_files.append(extract_path)\n                    self.logger.info(f\"Extracted: {member}\")\n            \n            return extracted_files\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to extract {zip_path.name}: {e}\")\n            raise\n    \n    def filter_file_by_state(self, input_file, state_code):\n        \"\"\"\n        Filter a single TXT file by state code using streaming (line-by-line)\n        \n        Args:\n            input_file: Path to input TXT file\n            state_code: State code to filter (e.g., 'CA', 'TX', 'FL')\n        \n        Returns:\n            Path to filtered output file\n        \"\"\"\n        start_time = time.time()\n        \n        output_filename = f\"{input_file.stem}_filtered_{state_code}{input_file.suffix}\"\n        output_file = self.filtered_dir / output_filename\n        \n        self.logger.info(f\"Filtering {input_file.name} for state: {state_code}\")\n        \n        records_processed = 0\n        records_kept = 0\n        header_line = None\n        state_column_index = None\n        \n        try:\n            with open(input_file, 'r', encoding='utf-8', errors='ignore') as infile, \\\n                 open(output_file, 'w', encoding='utf-8') as outfile:\n                \n                for line_num, line in enumerate(infile, 1):\n                    if line_num == 1:\n                        header_line = line\n                        outfile.write(line)\n                        \n                        headers = line.strip().split(self.delimiter)\n                        try:\n                            state_column_index = headers.index(self.state_code_column)\n                        except ValueError:\n                            self.logger.warning(f\"Column '{self.state_code_column}' not found in {input_file.name}. Using fallback strategy.\")\n                            state_column_index = None\n                        \n                        continue\n                    \n                    records_processed += 1\n                    \n                    if state_column_index is not None:\n                        columns = line.strip().split(self.delimiter)\n                        \n                        if len(columns) > state_column_index:\n                            record_state = columns[state_column_index].strip()\n                            \n                            if record_state == state_code:\n                                outfile.write(line)\n                                records_kept += 1\n                    else:\n                        if f\"{self.delimiter}{state_code}{self.delimiter}\" in line or \\\n                           line.startswith(f\"{state_code}{self.delimiter}\") or \\\n                           line.endswith(f\"{self.delimiter}{state_code}\"):\n                            outfile.write(line)\n                            records_kept += 1\n                    \n                    if records_processed % 100000 == 0:\n                        self.logger.log_filter_progress(input_file.name, records_processed, records_kept)\n            \n            duration = time.time() - start_time\n            \n            self.logger.info(f\"Filtering complete: {input_file.name}\")\n            self.logger.info(f\"Total processed: {records_processed}, Kept: {records_kept}, Duration: {duration:.2f}s\")\n            \n            return output_file\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to filter {input_file.name}: {e}\")\n            raise\n    \n    def filter_multiple_states(self, input_file):\n        \"\"\"\n        Filter a TXT file for all configured states\n        \n        Args:\n            input_file: Path to input TXT file\n        \n        Returns:\n            List of filtered file paths (one per state)\n        \"\"\"\n        filtered_files = []\n        \n        for state_code in self.states:\n            try:\n                filtered_file = self.filter_file_by_state(input_file, state_code)\n                filtered_files.append(filtered_file)\n            except Exception as e:\n                self.logger.error(f\"Failed to filter {input_file.name} for state {state_code}: {e}\")\n        \n        return filtered_files\n    \n    def compress_to_zip(self, files, output_zip_name):\n        \"\"\"\n        Compress filtered files into a ZIP archive\n        \n        Args:\n            files: List of file paths to compress\n            output_zip_name: Name of output ZIP file\n        \n        Returns:\n            Path to created ZIP file\n        \"\"\"\n        output_zip = self.filtered_dir / output_zip_name\n        \n        self.logger.info(f\"Creating ZIP archive: {output_zip_name}\")\n        \n        try:\n            with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                for file_path in files:\n                    if file_path.exists():\n                        zipf.write(file_path, file_path.name)\n                        self.logger.debug(f\"Added to ZIP: {file_path.name}\")\n            \n            self.logger.info(f\"ZIP archive created: {output_zip_name}\")\n            return output_zip\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to create ZIP {output_zip_name}: {e}\")\n            raise\n","size_bytes":6629},"src/utils/__init__.py":{"content":"from .logger import ETLLogger\nfrom .config_loader import ConfigLoader\n\n__all__ = ['ETLLogger', 'ConfigLoader']\n","size_bytes":111},"src/utils/config_loader.py":{"content":"import json\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n\nclass ConfigLoader:\n    \"\"\"Load and manage configuration from JSON and environment variables\"\"\"\n    \n    def __init__(self, config_file=\"config/config.json\"):\n        env_file = Path('config/.env')\n        if env_file.exists():\n            load_dotenv(dotenv_path=env_file)\n        else:\n            load_dotenv()\n        \n        self.config_file = Path(config_file)\n        self.config = self._load_config()\n        \n    def _load_config(self):\n        \"\"\"Load configuration from JSON file\"\"\"\n        if not self.config_file.exists():\n            raise FileNotFoundError(f\"Configuration file not found: {self.config_file}\")\n        \n        with open(self.config_file, 'r') as f:\n            config = json.load(f)\n        \n        config['spaces']['access_key'] = os.getenv('SPACES_ACCESS_KEY', config['spaces'].get('access_key', ''))\n        config['spaces']['secret_key'] = os.getenv('SPACES_SECRET_KEY', config['spaces'].get('secret_key', ''))\n        \n        config['ftp']['username'] = os.getenv('FTP_USERNAME', config['ftp'].get('username', ''))\n        config['ftp']['password'] = os.getenv('FTP_PASSWORD', config['ftp'].get('password', ''))\n        \n        return config\n    \n    def get_states(self):\n        \"\"\"Get list of states to filter\"\"\"\n        return self.config.get('states', [])\n    \n    def get_datasets(self):\n        \"\"\"Get list of datasets to process\"\"\"\n        return self.config.get('datasets', [])\n    \n    def get_ftp_config(self):\n        \"\"\"Get FTP configuration\"\"\"\n        return self.config.get('ftp', {})\n    \n    def get_spaces_config(self):\n        \"\"\"Get DigitalOcean Spaces configuration\"\"\"\n        return self.config.get('spaces', {})\n    \n    def get_schedule_time(self):\n        \"\"\"Get daily schedule time\"\"\"\n        return self.config.get('schedule', {}).get('daily_time', '02:00')\n    \n    def get_working_directories(self):\n        \"\"\"Get working directory paths\"\"\"\n        return self.config.get('directories', {})\n    \n    def get_state_code_column(self):\n        \"\"\"Get the column name for state code filtering\"\"\"\n        return self.config.get('filter_column', 'SitusStateCode')\n    \n    def get_file_delimiter(self):\n        \"\"\"Get the file delimiter (default: tab)\"\"\"\n        return self.config.get('file_delimiter', '\\t')\n","size_bytes":2355},"main.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nAttom Real Estate Data ETL Pipeline\nMain entry point for the application\n\"\"\"\n\nimport sys\nimport argparse\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent / 'src'))\n\nfrom etl_pipeline import AttomETLPipeline\nfrom scheduler import ETLScheduler\n\n\ndef run_once(config_file):\n    \"\"\"Run ETL pipeline once\"\"\"\n    print(\"Running ETL pipeline (single execution)...\\n\")\n    \n    try:\n        pipeline = AttomETLPipeline(config_file)\n        results = pipeline.run_all_datasets()\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"ETL PIPELINE EXECUTION COMPLETE\")\n        print(\"=\"*80)\n        print(f\"Total datasets: {results['total']}\")\n        print(f\"Successful: {results['success']}\")\n        print(f\"Failed: {results['failed']}\")\n        print(f\"Skipped: {results['skipped']}\")\n        print(f\"Total duration: {results['duration']:.2f} seconds\")\n        print(\"=\"*80)\n        \n        if results['failed'] > 0:\n            print(\"\\nWARNING: Some datasets failed to process. Check logs for details.\")\n            sys.exit(1)\n        else:\n            print(\"\\nAll datasets processed successfully!\")\n            sys.exit(0)\n            \n    except Exception as e:\n        print(f\"\\nFATAL ERROR: {e}\")\n        print(\"Check logs for details.\")\n        sys.exit(1)\n\n\ndef run_scheduler(config_file, run_immediately):\n    \"\"\"Run ETL pipeline with scheduler\"\"\"\n    print(\"Starting ETL scheduler...\\n\")\n    \n    try:\n        scheduler = ETLScheduler(config_file)\n        scheduler.start(run_immediately=run_immediately)\n    except KeyboardInterrupt:\n        print(\"\\nScheduler stopped by user.\")\n        sys.exit(0)\n    except Exception as e:\n        print(f\"\\nFATAL ERROR: {e}\")\n        sys.exit(1)\n\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Attom Real Estate Data ETL Pipeline',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  %(prog)s run                    # Run ETL pipeline once\n  %(prog)s schedule               # Run with daily scheduler\n  %(prog)s schedule --now         # Run immediately, then on schedule\n  %(prog)s run --config custom.json  # Use custom config file\n        \"\"\"\n    )\n    \n    parser.add_argument(\n        'mode',\n        choices=['run', 'schedule'],\n        help='Execution mode: \"run\" for single execution, \"schedule\" for automated daily runs'\n    )\n    \n    parser.add_argument(\n        '--config',\n        default='config/config.json',\n        help='Path to configuration file (default: config/config.json)'\n    )\n    \n    parser.add_argument(\n        '--now',\n        action='store_true',\n        help='Run ETL immediately when using schedule mode (in addition to scheduled runs)'\n    )\n    \n    args = parser.parse_args()\n    \n    print(\"=\"*80)\n    print(\"ATTOM REAL ESTATE DATA ETL PIPELINE\")\n    print(\"=\"*80)\n    print(f\"Mode: {args.mode}\")\n    print(f\"Config: {args.config}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    if args.mode == 'run':\n        run_once(args.config)\n    elif args.mode == 'schedule':\n        run_scheduler(args.config, args.now)\n\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":3161},"src/loaders/__init__.py":{"content":"from .spaces_uploader import SpacesUploader\n\n__all__ = ['SpacesUploader']\n","size_bytes":74},"src/transformers/__init__.py":{"content":"from .state_filter import StateFilter\n\n__all__ = ['StateFilter']\n","size_bytes":65},"src/extractors/ftp_downloader.py":{"content":"import os\nimport ftplib\nimport socket\nimport time\nfrom pathlib import Path\n\nclass FTPDownloader:\n    \"\"\"Handles downloading files from ATTOM FTPS/FTP servers.\"\"\"\n\n    def __init__(self, logger, ftp_config):\n        \"\"\"\n        Initialize the FTP downloader.\n\n        Args:\n            logger: ETLLogger instance for consistent logging\n            ftp_config: dict containing FTP credentials and settings\n        \"\"\"\n        self.logger = logger\n        self.ftp_config = ftp_config\n        self.download_dir = Path(ftp_config.get('download_dir', 'data/downloads'))\n        self.download_dir.mkdir(parents=True, exist_ok=True)\n\n    def _connect(self):\n        \"\"\"Connect to FTP or FTPS server.\"\"\"\n        server = self.ftp_config.get('host')\n        username = self.ftp_config.get('username')\n        password = self.ftp_config.get('password')\n        use_ftps = self.ftp_config.get('use_ftps', True)\n\n        if not server or not username or not password:\n            raise ValueError(\"Missing FTP configuration values\")\n\n        self.logger.info(f\"Connecting to {'FTPS' if use_ftps else 'FTP'} server: {server}\")\n\n        if use_ftps:\n            ftp = ftplib.FTP_TLS(server)\n            ftp.login(username, password)\n            ftp.prot_p()  # Secure data connection\n        else:\n            ftp = ftplib.FTP(server)\n            ftp.login(username, password)\n\n        self.logger.info(\"Connected successfully to FTP server\")\n        return ftp\n\n    def download_dataset(self, dataset_config):\n        \"\"\"\n        Download files for a dataset.\n\n        Args:\n            dataset_config: dict with dataset configuration\n        Returns:\n            List of downloaded file paths\n        \"\"\"\n        dataset_name = dataset_config.get('name')\n        remote_paths = dataset_config.get('urls', [])\n        downloaded_files = []\n\n        if not remote_paths:\n            self.logger.warning(f\"No URLs provided for dataset {dataset_name}\")\n            return []\n\n        try:\n            ftp = self._connect()\n            ftp.set_pasv(True)\n\n            for remote_path in remote_paths:\n                file_name = os.path.basename(remote_path)\n                local_path = self.download_dir / file_name\n                self.logger.info(f\"Downloading {remote_path} to {local_path}\")\n\n                try:\n                    with open(local_path, \"wb\") as f:\n                        ftp.retrbinary(f\"RETR {remote_path}\", f.write)\n                    self.logger.info(f\"Downloaded: {local_path}\")\n                    downloaded_files.append(local_path)\n                except Exception as e:\n                    self.logger.error(f\"Error downloading {remote_path}: {e}\")\n                    continue\n\n            ftp.quit()\n            return downloaded_files\n\n        except Exception as e:\n            self.logger.error(f\"FTP download failed for {dataset_name}: {e}\")\n            return []\n\n    def download_test_files(self):\n        \"\"\"Test FTP connection.\"\"\"\n        self.logger.info(\"Testing FTP connection...\")\n        try:\n            ftp = self._connect()\n            ftp.set_pasv(True)\n            ftp.cwd('/')\n            files = ftp.nlst()\n            self.logger.info(f\"Files in root: {files[:5]} (showing up to 5)\")\n            ftp.quit()\n        except Exception as e:\n            self.logger.error(f\"FTP test failed: {e}\")\n","size_bytes":3334},"src/extractors/__init__.py":{"content":"from .ftp_downloader import FTPDownloader\n\n__all__ = ['FTPDownloader']\n","size_bytes":71},"src/scheduler.py":{"content":"import schedule\nimport time\nfrom datetime import datetime\nfrom etl_pipeline import AttomETLPipeline\n\n\nclass ETLScheduler:\n    \"\"\"Automated daily scheduler for ETL pipeline\"\"\"\n    \n    def __init__(self, config_file=\"config/config.json\"):\n        self.config_file = config_file\n        self.pipeline = None\n        self.schedule_time = \"02:00\"\n        \n        self._load_schedule_config()\n        \n    def _load_schedule_config(self):\n        \"\"\"Load schedule configuration\"\"\"\n        try:\n            from utils.config_loader import ConfigLoader\n            config_loader = ConfigLoader(self.config_file)\n            self.schedule_time = config_loader.get_schedule_time()\n            print(f\"Scheduler initialized - Daily run scheduled at {self.schedule_time}\")\n        except Exception as e:\n            print(f\"Warning: Could not load schedule config, using default time {self.schedule_time}: {e}\")\n    \n    def run_etl_job(self):\n        \"\"\"Execute the ETL pipeline job\"\"\"\n        print(f\"\\n{'='*80}\")\n        print(f\"ETL Job triggered at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        print(f\"{'='*80}\\n\")\n        \n        try:\n            self.pipeline = AttomETLPipeline(self.config_file)\n            results = self.pipeline.run_all_datasets()\n            \n            print(f\"\\n{'='*80}\")\n            print(f\"ETL Job completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n            print(f\"Success: {results['success']}, Failed: {results['failed']}, Skipped: {results['skipped']}\")\n            print(f\"{'='*80}\\n\")\n            \n        except Exception as e:\n            print(f\"ERROR: ETL job failed: {e}\")\n    \n    def start(self, run_immediately=False):\n        \"\"\"\n        Start the scheduler\n        \n        Args:\n            run_immediately: If True, run ETL job immediately on start\n        \"\"\"\n        print(f\"Starting ETL Scheduler...\")\n        print(f\"Scheduled to run daily at {self.schedule_time}\")\n        \n        schedule.every().day.at(self.schedule_time).do(self.run_etl_job)\n        \n        if run_immediately:\n            print(\"Running ETL job immediately...\")\n            self.run_etl_job()\n        \n        print(\"Scheduler is running. Press Ctrl+C to stop.\")\n        \n        try:\n            while True:\n                schedule.run_pending()\n                time.sleep(60)\n        except KeyboardInterrupt:\n            print(\"\\nScheduler stopped by user.\")\n\n\ndef main():\n    \"\"\"Main entry point for scheduler\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Attom ETL Scheduler')\n    parser.add_argument('--config', default='config/config.json', help='Path to config file')\n    parser.add_argument('--now', action='store_true', help='Run ETL immediately on start')\n    \n    args = parser.parse_args()\n    \n    scheduler = ETLScheduler(args.config)\n    scheduler.start(run_immediately=args.now)\n\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":2917},"appsnote.md":{"content":"# Attom Real Estate Data ETL Pipeline\n\n## Project Overview\n\nThis is a production-ready Python ETL (Extract-Transform-Load) pipeline designed to process over 142 GB of Attom nationwide real estate data. The application automatically downloads data from FTP sources, filters it by state codes, and uploads the processed files to DigitalOcean Spaces for consumption by .NET applications.\n\n## Current State\n\n**Status**: Fully functional and ready for deployment\n\nThe application includes:\n- ✅ Complete ETL pipeline with Extract, Transform, and Load phases\n- ✅ Memory-efficient streaming for 2GB+ files\n- ✅ FTP/HTTP download module\n- ✅ State-based filtering system\n- ✅ DigitalOcean Spaces integration\n- ✅ Comprehensive logging system\n- ✅ Daily scheduling with Python `schedule` library\n- ✅ Full documentation and deployment guides\n\n## Architecture\n\n### High-Priority Datasets (Currently Configured)\n1. **Assessor** - Property assessment data\n2. **AVM** - Automated Valuation Model data\n3. **Parcel** - Parcel data (via Jetstream)\n4. **PROPERTYTOBOUNDARYMATCH_PARCEL** - Property boundary matching\n\n### Lower-Priority Datasets (Disabled by Default)\n5. **Recorder** - Recording data\n6. **PreForeclosure** - Pre-foreclosure data\n\n### Project Structure\n```\n├── src/\n│   ├── extractors/          # FTP/HTTP download modules\n│   ├── transformers/        # State filtering and data processing\n│   ├── loaders/             # DigitalOcean Spaces uploader\n│   ├── utils/               # Logger and configuration loader\n│   ├── etl_pipeline.py      # Main ETL orchestrator\n│   └── scheduler.py         # Daily scheduling system\n├── config/\n│   ├── config.json          # Main configuration (states, datasets, URLs)\n│   └── .env.example         # Credentials template\n├── data/                    # Working directories (downloads, extracted, filtered)\n├── logs/                    # ETL process logs\n├── main.py                  # Application entry point\n├── validate_setup.py        # Setup validation script\n├── README.md                # User documentation\n└── DEPLOYMENT.md            # Production deployment guide\n```\n\n## Key Features\n\n### Memory Efficiency\n- Processes 2GB+ files line-by-line without loading into memory\n- Streams data to avoid memory exhaustion on large datasets\n\n### State Filtering\n- Configurable state list (e.g., CA, TX, FL)\n- Filters by `SitusStateCode` column\n- Preserves original file format and headers\n\n### DigitalOcean Spaces Integration\n- Date-based folder structure: `2025-10-29/Assessor_filtered_CA.zip`\n- Automatic upload after processing\n- S3-compatible boto3 integration\n\n### Logging\n- Comprehensive logs tracking:\n  - Download progress\n  - Record counts (processed vs. kept)\n  - Processing duration\n  - Errors and warnings\n- Logs uploaded to Spaces for remote monitoring\n\n### Scheduling\n- Daily execution at configurable time (default: 2:00 AM)\n- Compatible with systemd, cron, or pm2\n- Manual execution also supported\n\n## Quick Start\n\n### 1. Configure Credentials\n\nCreate `config/.env` file:\n```bash\nSPACES_ACCESS_KEY=your_digitalocean_spaces_key\nSPACES_SECRET_KEY=your_digitalocean_spaces_secret\nFTP_USERNAME=your_attom_ftp_username\nFTP_PASSWORD=your_attom_ftp_password\n```\n\n### 2. Update Configuration\n\nEdit `config/config.json`:\n- Update `states` array with target state codes\n- Configure `datasets[].urls` with actual Attom FTP URLs\n- Set `spaces.bucket_name` to your DigitalOcean Spaces bucket\n- Adjust `schedule.daily_time` as needed\n\n### 3. Run the Application\n\n```bash\n# Validate setup\npython validate_setup.py\n\n# Run ETL pipeline once\npython main.py run\n\n# Run with daily scheduler\npython main.py schedule\n\n# Run immediately and then on schedule\npython main.py schedule --now\n```\n\n## Configuration\n\n### State Codes\nCurrently configured states: **CA, TX, FL**\n\nTo add more states, edit `config/config.json`:\n```json\n\"states\": [\"CA\", \"TX\", \"FL\", \"NY\", \"IL\", \"OH\"]\n```\n\n### Dataset Configuration\nEach dataset in `config.json` has:\n- `name`: Dataset identifier\n- `enabled`: true/false to enable/disable\n- `urls`: Array of download URLs (FTP or HTTP)\n- `description`: Dataset description\n\n### Schedule\nDaily execution time is configured in `config.json`:\n```json\n\"schedule\": {\n  \"daily_time\": \"02:00\"\n}\n```\n\n## Deployment\n\nSee `DEPLOYMENT.md` for complete production deployment instructions including:\n- DigitalOcean Droplet setup\n- Systemd service configuration\n- Monitoring and maintenance\n- Troubleshooting guide\n\n## Recent Changes\n\n**2025-10-29**: Initial implementation\n- Complete ETL pipeline implementation\n- All high-priority datasets configured\n- Documentation and deployment guides created\n- Setup validation script added\n\n## User Preferences\n\nNone specified yet.\n\n## Technical Stack\n\n- **Language**: Python 3.11\n- **Libraries**:\n  - boto3 (DigitalOcean Spaces/S3)\n  - requests (HTTP downloads)\n  - schedule (job scheduling)\n  - python-dotenv (environment variables)\n- **Hosting**: DigitalOcean Droplet (production)\n- **Storage**: DigitalOcean Spaces (S3-compatible)\n- **Scheduling**: Python schedule + systemd/cron/pm2\n\n## Next Steps\n\n1. **Configure credentials**: Add actual credentials to `config/.env`\n2. **Update FTP URLs**: Replace example URLs in `config.json` with actual Attom FTP endpoints\n3. **Test locally**: Run `python main.py run` to test the pipeline\n4. **Deploy to droplet**: Follow `DEPLOYMENT.md` for production deployment\n5. **Monitor logs**: Check `logs/` directory and DigitalOcean Spaces for outputs\n\n## Future Enhancements\n\n- Email notifications for job failures\n- Web dashboard for monitoring job status\n- Parallel processing for multiple datasets\n- Incremental updates (only download changed files)\n- Data validation and integrity checks\n- Retry logic with exponential backoff\n\n## Notes\n\n- The application is designed to run on a DigitalOcean Droplet but can run on any Linux server\n- All temporary files are cleaned up after processing to save disk space\n- Logs are preserved locally and uploaded to Spaces\n- The .NET Assessor application can directly consume the filtered ZIPs from Spaces\n","size_bytes":6184}},"version":2}